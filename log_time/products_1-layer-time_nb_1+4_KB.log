main start at this time 1675445761.767014
-----------------------------------------before load data 
 Nvidia-smi: 0.166015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.166015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

in feats:  100
----------------------------------------- after block subtensor loading 
 Nvidia-smi: 1.6796875 GB
    Memory Allocated: 0.3182086944580078  GigaBytes
Max Memory Allocated: 0.3182086944580078  GigaBytes

------------------------------------ before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 1.787109375 GB
    Memory Allocated: 0.34946632385253906  GigaBytes
Max Memory Allocated: 0.34947919845581055  GigaBytes


current graph nodes  848910
current graph dst nodes  196571
current graph edges  1949041
---------- degree  tensor([1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0', dtype=torch.int32)

degs = graph.in_degrees() time :  0.00041747093200683594
nodes = graph.dstnodes() time :  5.1021575927734375e-05
ntid = graph.get_ntype_id_from_dst(ntype) time :  5.7220458984375e-06
msgdata = Frame(msgdata) time :  4.76837158203125e-07
get ready time :  0.0004994869232177734
len val:  196571
----------- core.py torch.sort() replace to OrderedDict time:  0.08161234855651855
eqidx = F.nonzero_1d(F.equal(sorted_val, v))  0.0018193721771240234
bucketing time:  0.084075927734375
len node_bkt:  173
len eid_bkt:  173
ndata_bkt = dstdata.subframe(node_bkt):  5.435943603515625e-05
eid_bkt generation:  0.21690154075622559
msgdata_bkt = msgdata.subframe(eid_bkt):  4.124641418457031e-05
msgdata_bkt generation:  0.21694278717041016
msgdata_bkt for loop:  0.0002715587615966797
NodeBatch generation:  7.867813110351562e-06
func(nbatch):  0.003961086273193359
len node_bkt:  221
len eid_bkt:  442
ndata_bkt = dstdata.subframe(node_bkt):  5.340576171875e-05
eid_bkt generation:  0.0004410743713378906
msgdata_bkt = msgdata.subframe(eid_bkt):  2.4080276489257812e-05
msgdata_bkt generation:  0.00046515464782714844
msgdata_bkt for loop:  0.0001342296600341797
NodeBatch generation:  6.9141387939453125e-06
func(nbatch):  0.0005879402160644531
len node_bkt:  374
len eid_bkt:  1122
ndata_bkt = dstdata.subframe(node_bkt):  3.4809112548828125e-05
eid_bkt generation:  0.0003173351287841797
msgdata_bkt = msgdata.subframe(eid_bkt):  1.811981201171875e-05
msgdata_bkt generation:  0.00033545494079589844
msgdata_bkt for loop:  0.00012087821960449219
NodeBatch generation:  4.5299530029296875e-06
func(nbatch):  0.0010228157043457031
len node_bkt:  456
len eid_bkt:  1824
ndata_bkt = dstdata.subframe(node_bkt):  3.3855438232421875e-05
eid_bkt generation:  0.00029850006103515625
msgdata_bkt = msgdata.subframe(eid_bkt):  1.8835067749023438e-05
msgdata_bkt generation:  0.0003173351287841797
msgdata_bkt for loop:  0.00011348724365234375
NodeBatch generation:  4.291534423828125e-06
func(nbatch):  0.0013959407806396484
len node_bkt:  445
len eid_bkt:  2225
ndata_bkt = dstdata.subframe(node_bkt):  3.337860107421875e-05
eid_bkt generation:  0.00030112266540527344
msgdata_bkt = msgdata.subframe(eid_bkt):  1.8596649169921875e-05
msgdata_bkt generation:  0.0003197193145751953
msgdata_bkt for loop:  0.00011348724365234375
NodeBatch generation:  4.5299530029296875e-06
func(nbatch):  0.0017037391662597656
len node_bkt:  550
len eid_bkt:  3300
ndata_bkt = dstdata.subframe(node_bkt):  3.2901763916015625e-05
eid_bkt generation:  0.00029850006103515625
msgdata_bkt = msgdata.subframe(eid_bkt):  1.9073486328125e-05
msgdata_bkt generation:  0.00031757354736328125
msgdata_bkt for loop:  0.00011324882507324219
NodeBatch generation:  4.291534423828125e-06
func(nbatch):  0.0019516944885253906
len node_bkt:  655
len eid_bkt:  4585
ndata_bkt = dstdata.subframe(node_bkt):  3.337860107421875e-05
eid_bkt generation:  0.00030994415283203125
msgdata_bkt = msgdata.subframe(eid_bkt):  1.9073486328125e-05
msgdata_bkt generation:  0.00032901763916015625
msgdata_bkt for loop:  0.000118255615234375
NodeBatch generation:  4.0531158447265625e-06
func(nbatch):  0.002516031265258789
len node_bkt:  512
len eid_bkt:  4096
ndata_bkt = dstdata.subframe(node_bkt):  3.337860107421875e-05
eid_bkt generation:  0.000308990478515625
msgdata_bkt = msgdata.subframe(eid_bkt):  1.7404556274414062e-05
msgdata_bkt generation:  0.00032639503479003906
msgdata_bkt for loop:  0.0001049041748046875
NodeBatch generation:  4.291534423828125e-06
func(nbatch):  0.002480030059814453
len node_bkt:  576
len eid_bkt:  5184
ndata_bkt = dstdata.subframe(node_bkt):  3.337860107421875e-05
eid_bkt generation:  0.00030922889709472656
msgdata_bkt = msgdata.subframe(eid_bkt):  1.8835067749023438e-05
msgdata_bkt generation:  0.00032806396484375
msgdata_bkt for loop:  0.00011706352233886719
NodeBatch generation:  4.291534423828125e-06
func(nbatch):  0.0028367042541503906
----NodeBatch generation time:  0.2402799129486084
----Frame generation time:  1.2874603271484375e-05
---- merge nodes time:  0.0006968975067138672
invoke_udf_reduce time:  0.33109092712402344
------------------------------------- after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 2.904296875 GB
    Memory Allocated: 0.6248698234558105  GigaBytes
Max Memory Allocated: 1.3623805046081543  GigaBytes

degree: tensor([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32) # of output: 3962 ratio: 0.02015556719963779 bucket_loss : tensor(0.0948)
------------------------------------- after loss backward 
 Nvidia-smi: 2.9140625 GB
    Memory Allocated: 0.3849453926086426  GigaBytes
Max Memory Allocated: 1.3623805046081543  GigaBytes

----------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 2.9140625 GB
    Memory Allocated: 0.35068416595458984  GigaBytes
Max Memory Allocated: 1.3623805046081543  GigaBytes


current graph nodes  848910
current graph dst nodes  196571
current graph edges  1949041
---------- degree  tensor(10)

degs = graph.in_degrees() time :  0.0003714561462402344
nodes = graph.dstnodes() time :  9.822845458984375e-05
ntid = graph.get_ntype_id_from_dst(ntype) time :  1.0967254638671875e-05
msgdata = Frame(msgdata) time :  7.152557373046875e-07
get ready time :  0.0005314350128173828
len val:  196571
----------- core.py torch.sort() replace to OrderedDict time:  0.10248827934265137
eqidx = F.nonzero_1d(F.equal(sorted_val, v))  0.00013685226440429688
tmp = F.gather_row(idx, eqidx)  3.0279159545898438e-05
N = math.ceil(len(tmp)/num_split)  1.5497207641601562e-05
unique_val = np.asarray(list([degree.item()]))  3.910064697265625e-05
step:  0
bucketing time:  0.10327315330505371
len node_bkt:  48153
len eid_bkt:  481530
ndata_bkt = dstdata.subframe(node_bkt):  5.555152893066406e-05
eid_bkt generation:  0.002481222152709961
msgdata_bkt = msgdata.subframe(eid_bkt):  3.886222839355469e-05
msgdata_bkt generation:  0.0025200843811035156
msgdata_bkt for loop:  0.0011675357818603516
NodeBatch generation:  5.9604644775390625e-06
func(nbatch):  0.010089635848999023
----NodeBatch generation time:  0.013927221298217773
----Frame generation time:  1.621246337890625e-05
---- merge nodes time:  0.0002052783966064453
invoke_udf_reduce time:  0.11828494071960449
------------------------------------after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 5.740234375 GB
    Memory Allocated: 3.252840042114258  GigaBytes
Max Memory Allocated: 4.042263507843018  GigaBytes

degree: tensor(10) # of output: 48153 ratio: 0.2449649236153858 bucket_loss : tensor(1.1749)
------------------------------------ after loss backward 
 Nvidia-smi: 5.740234375 GB
    Memory Allocated: 0.393892765045166  GigaBytes
Max Memory Allocated: 4.042263507843018  GigaBytes

----------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 5.740234375 GB
    Memory Allocated: 0.393892765045166  GigaBytes
Max Memory Allocated: 4.042263507843018  GigaBytes


current graph nodes  848910
current graph dst nodes  196571
current graph edges  1949041
---------- degree  tensor(10)

degs = graph.in_degrees() time :  0.00018262863159179688
nodes = graph.dstnodes() time :  3.910064697265625e-05
ntid = graph.get_ntype_id_from_dst(ntype) time :  4.291534423828125e-06
msgdata = Frame(msgdata) time :  2.384185791015625e-07
get ready time :  0.0002484321594238281
len val:  196571
----------- core.py torch.sort() replace to OrderedDict time:  0.08224821090698242
eqidx = F.nonzero_1d(F.equal(sorted_val, v))  0.00012350082397460938
tmp = F.gather_row(idx, eqidx)  3.0279159545898438e-05
N = math.ceil(len(tmp)/num_split)  1.430511474609375e-05
unique_val = np.asarray(list([degree.item()]))  3.504753112792969e-05
step:  1
bucketing time:  0.08300352096557617
len node_bkt:  48153
len eid_bkt:  481530
ndata_bkt = dstdata.subframe(node_bkt):  5.6743621826171875e-05
eid_bkt generation:  0.002475738525390625
msgdata_bkt = msgdata.subframe(eid_bkt):  3.647804260253906e-05
msgdata_bkt generation:  0.002512216567993164
msgdata_bkt for loop:  0.0006480216979980469
NodeBatch generation:  5.7220458984375e-06
func(nbatch):  0.001560211181640625
----NodeBatch generation time:  0.0048673152923583984
----Frame generation time:  1.5020370483398438e-05
---- merge nodes time:  0.00019431114196777344
invoke_udf_reduce time:  0.08849167823791504
------------------------------------after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 5.919921875 GB
    Memory Allocated: 3.2616305351257324  GigaBytes
Max Memory Allocated: 4.0854716300964355  GigaBytes

degree: tensor(10) # of output: 48153 ratio: 0.2449649236153858 bucket_loss : tensor(1.1762)
------------------------------------ after loss backward 
 Nvidia-smi: 5.919921875 GB
    Memory Allocated: 0.393892765045166  GigaBytes
Max Memory Allocated: 4.0854716300964355  GigaBytes

----------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 5.919921875 GB
    Memory Allocated: 0.393892765045166  GigaBytes
Max Memory Allocated: 4.0854716300964355  GigaBytes


current graph nodes  848910
current graph dst nodes  196571
current graph edges  1949041
---------- degree  tensor(10)

degs = graph.in_degrees() time :  0.0001995563507080078
nodes = graph.dstnodes() time :  5.459785461425781e-05
ntid = graph.get_ntype_id_from_dst(ntype) time :  5.0067901611328125e-06
msgdata = Frame(msgdata) time :  4.76837158203125e-07
get ready time :  0.00028514862060546875
len val:  196571
----------- core.py torch.sort() replace to OrderedDict time:  0.08513498306274414
eqidx = F.nonzero_1d(F.equal(sorted_val, v))  0.00011873245239257812
tmp = F.gather_row(idx, eqidx)  2.9802322387695312e-05
N = math.ceil(len(tmp)/num_split)  1.33514404296875e-05
unique_val = np.asarray(list([degree.item()]))  3.719329833984375e-05
step:  2
bucketing time:  0.08589410781860352
len node_bkt:  48153
len eid_bkt:  481530
ndata_bkt = dstdata.subframe(node_bkt):  5.507469177246094e-05
eid_bkt generation:  0.0025277137756347656
msgdata_bkt = msgdata.subframe(eid_bkt):  3.910064697265625e-05
msgdata_bkt generation:  0.002566814422607422
msgdata_bkt for loop:  0.0006551742553710938
NodeBatch generation:  5.4836273193359375e-06
func(nbatch):  0.0010623931884765625
----NodeBatch generation time:  0.004429817199707031
----Frame generation time:  1.5020370483398438e-05
---- merge nodes time:  0.00020647048950195312
invoke_udf_reduce time:  0.09101223945617676
------------------------------------after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 5.919921875 GB
    Memory Allocated: 3.2619175910949707  GigaBytes
Max Memory Allocated: 4.0854716300964355  GigaBytes

degree: tensor(10) # of output: 48153 ratio: 0.2449649236153858 bucket_loss : tensor(1.1706)
------------------------------------ after loss backward 
 Nvidia-smi: 5.919921875 GB
    Memory Allocated: 0.393892765045166  GigaBytes
Max Memory Allocated: 4.0854716300964355  GigaBytes

----------------------------------- before batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 5.919921875 GB
    Memory Allocated: 0.393892765045166  GigaBytes
Max Memory Allocated: 4.0854716300964355  GigaBytes


current graph nodes  848910
current graph dst nodes  196571
current graph edges  1949041
---------- degree  tensor(10)

degs = graph.in_degrees() time :  0.0001537799835205078
nodes = graph.dstnodes() time :  3.218650817871094e-05
ntid = graph.get_ntype_id_from_dst(ntype) time :  3.5762786865234375e-06
msgdata = Frame(msgdata) time :  0.0
get ready time :  0.0002079010009765625
len val:  196571
----------- core.py torch.sort() replace to OrderedDict time:  0.08837008476257324
eqidx = F.nonzero_1d(F.equal(sorted_val, v))  0.00012683868408203125
tmp = F.gather_row(idx, eqidx)  2.9802322387695312e-05
N = math.ceil(len(tmp)/num_split)  1.430511474609375e-05
unique_val = np.asarray(list([degree.item()]))  3.4809112548828125e-05
step:  3
bucketing time:  0.08907771110534668
len node_bkt:  48150
len eid_bkt:  481500
ndata_bkt = dstdata.subframe(node_bkt):  5.626678466796875e-05
eid_bkt generation:  0.002727985382080078
msgdata_bkt = msgdata.subframe(eid_bkt):  3.790855407714844e-05
msgdata_bkt generation:  0.0027658939361572266
msgdata_bkt for loop:  0.0006604194641113281
NodeBatch generation:  5.9604644775390625e-06
func(nbatch):  0.0011148452758789062
----NodeBatch generation time:  0.004693746566772461
----Frame generation time:  1.5020370483398438e-05
---- merge nodes time:  0.000209808349609375
invoke_udf_reduce time:  0.09435153007507324
------------------------------------after batch_pred = model(blocks, batch_inputs, deg.to(device)) 
 Nvidia-smi: 5.919921875 GB
    Memory Allocated: 3.2616043090820312  GigaBytes
Max Memory Allocated: 4.0854716300964355  GigaBytes

degree: tensor(10) # of output: 48150 ratio: 0.24494966195420484 bucket_loss : tensor(1.1753)
------------------------------------ after loss backward 
 Nvidia-smi: 5.919921875 GB
    Memory Allocated: 0.3938922882080078  GigaBytes
Max Memory Allocated: 4.0854716300964355  GigaBytes

-------------------------------------------------------------------------------loss_sum  :  tensor(4.7918)

